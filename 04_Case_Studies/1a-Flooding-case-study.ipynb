{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding a flooding event using OPERA DSWx-HLS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Heavy rains severely impacted Southeast Texas in May 2024 [[1]](https://www.texastribune.org/2024/05/03/texas-floods-weather-harris-county/), resulting in flooding and causing significant damage to property and human life [[2]](https://www.texastribune.org/series/east-texas-floods-2024/). In this notebook, we will retrieve [OPERA DSWx-HLS](https://d2pn8kiwq2w21t.cloudfront.net/documents/ProductSpec_DSWX_URS309746.pdf) data associated to understand the extent of flooding and damage, and visualize data from before, during, and after the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\") # suppress PySTAC warnings\n",
    "\n",
    "# GIS imports\n",
    "import rasterio\n",
    "import rioxarray\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.warp import transform_bounds\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "import xyzservices.providers as xyz\n",
    "from shapely.geometry import Point\n",
    "from osgeo import gdal\n",
    "\n",
    "# Misc imports\n",
    "import hvplot.xarray  # noqa\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study location\n",
    "livingston_tx_lonlat = (-95.09, 30.69) # (lon, lat) form\n",
    "\n",
    "ref_crs = CRS.from_epsg(4326)\n",
    "dst_crs = CRS.from_epsg(3857)\n",
    "map_bounds = transform_bounds(ref_crs, dst_crs, *Point(*livingston_tx_lonlat).buffer(10).bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize location of area of study\n",
    "livingston_tx = gv.Points([livingston_tx_lonlat])\n",
    "\n",
    "basemap = gv.tile_sources.OSM\n",
    "plot = (livingston_tx*basemap).opts(\n",
    "    opts.Points(\n",
    "        color='red',\n",
    "        alpha=0.75,\n",
    "        size=25,\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xlim=(map_bounds[0], map_bounds[2]),\n",
    "        ylim=(map_bounds[1], map_bounds[3]))\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The flooding event primarily happened during 04/30 - 05/02\n",
    "# We will search for data before and after the event\n",
    "start_date = datetime(year=2024, month=4, day=1)\n",
    "stop_date = datetime(year=2024, month=5, day=31)\n",
    "date_range = f'{start_date.strftime(\"%Y-%m-%d\")}/{stop_date.strftime(\"%Y-%m-%d\")}'\n",
    "\n",
    "# We open a client instance to search for data, and retrieve relevant data records\n",
    "STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "\n",
    "# Setup PySTAC client\n",
    "# POCLOUD refers to the PO DAAC cloud environment that hosts earth observation data\n",
    "catalog = Client.open(f'{STAC_URL}/POCLOUD/') \n",
    "\n",
    "# Setup PySTAC client\n",
    "provider_cat = Client.open(STAC_URL)\n",
    "catalog = Client.open(f'{STAC_URL}/POCLOUD/')\n",
    "collections = [\"OPERA_L3_DSWX-HLS_V1\"]\n",
    "\n",
    "search_opts = {\n",
    "    'bbox' : Point(*livingston_tx_lonlat).buffer(0.01).bounds, \n",
    "    'collections': collections,\n",
    "    'datetime' : date_range,\n",
    "}\n",
    "\n",
    "search = catalog.search(**search_opts)\n",
    "results = list(search.items_as_dicts())\n",
    "print(f\"Number of tiles found intersecting given AOI: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_to_df(results, layer_name):\n",
    "        '''\n",
    "        method that takes in search results from a PySTAC Earthdata query and loads the data into a pandas dataframe\n",
    "        the data frame has columns of date of acquisition, data URLs, and tile ID\n",
    "        '''\n",
    "\n",
    "        times = pd.DatetimeIndex([result['properties']['datetime'] for result in results]) # parse of timestamp for each result\n",
    "        data = {'hrefs': [value['href'] for result in results for key, value in result['assets'].items() if layer_name in key],  # parse out links only to DIST-STATUS data layer\n",
    "                'tile_id': [value['href'].split('/')[-1].split('_')[3] for result in results for key, value in result['assets'].items() if layer_name in key]}\n",
    "\n",
    "        # Construct pandas dataframe to summarize granules from search results\n",
    "        granules = pd.DataFrame(index=times, data=data)\n",
    "        granules.index.name = 'times'\n",
    "\n",
    "        return granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granules = search_to_df(results=results, layer_name='0_B01_WTR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now filter the dataframe to restrict our results to a single tile_id\n",
    "granules = granules[granules.tile_id == 'T15RTQ']\n",
    "granules.sort_index(inplace=True)\n",
    "len(granules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_to_dataset(granule_dataframe):\n",
    "    '''method that takes in a list of OPERA tile URLs and returns an xarray dataset with dimensions\n",
    "    latitude, longitude and time'''\n",
    "\n",
    "    dataset_list = []\n",
    "    \n",
    "    for i, row in granule_dataframe.iterrows():\n",
    "        with rasterio.open(row.hrefs) as ds:\n",
    "            # extract CRS string\n",
    "            crs = str(ds.crs).split(':')[-1]\n",
    "\n",
    "            # extract the image spatial extent (xmin, ymin, xmax, ymax)\n",
    "            xmin, ymin, xmax, ymax = ds.bounds\n",
    "\n",
    "            # the x and y resolution of the image is available in image metadata\n",
    "            x_res = np.abs(ds.transform[0])\n",
    "            y_res = np.abs(ds.transform[4])\n",
    "\n",
    "            # read the data \n",
    "            img = ds.read()\n",
    "\n",
    "            # Ensure img has three dimensions (bands, y, x)\n",
    "            if img.ndim == 2:\n",
    "                img = np.expand_dims(img, axis=0) \n",
    "\n",
    "        lon = np.arange(xmin, xmax, x_res)\n",
    "        lat = np.arange(ymax, ymin, -y_res)\n",
    "\n",
    "        da = xr.DataArray(\n",
    "            data=img,\n",
    "            dims=[\"band\", \"lat\", \"lon\"],\n",
    "            coords=dict(\n",
    "                lon=([\"lon\"], lon),\n",
    "                lat=([\"lat\"], lat),\n",
    "                time=i,\n",
    "                band=np.arange(img.shape[0])\n",
    "            ),\n",
    "            attrs=dict(\n",
    "                description=\"OPERA DSWx B01\",\n",
    "                units=None,\n",
    "            ),\n",
    "        )\n",
    "        da.rio.write_crs(crs, inplace=True)\n",
    "\n",
    "        dataset_list.append(da)\n",
    "    return xr.concat(dataset_list, dim='time').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= urls_to_dataset(granules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define a colormap\n",
    "COLORS = [(150, 150, 150, 0.1)]*256 # setting all values to gray with low opacity\n",
    "COLORS[0] = (0, 255, 0, 0.1) # Setting not water class to green\n",
    "COLORS[1] = (0, 0, 255, 1) # Open surface water\n",
    "COLORS[2] = (0, 0, 255, 1) # Partial surface water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset.hvplot.image(title = 'DSWx data for May 2024 Texas floods',\n",
    "                            x='lon', y='lat', \n",
    "                            project=True, rasterize=True,\n",
    "                            cmap=COLORS, \n",
    "                            colorbar=False,\n",
    "                            widget_location='bottom',\n",
    "                            tiles = gv.tile_sources.OSM,\n",
    "                            xlabel='Longitude (degrees)',ylabel='Latitude (degrees)',\n",
    "                            fontscale=1.25, frame_width=1000, frame_height=1000)\n",
    "\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
